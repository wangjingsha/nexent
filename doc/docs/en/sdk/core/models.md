# Nexent Model Architecture

Nexent provides a comprehensive model architecture supporting multiple AI model types through OpenAI-compatible interfaces. The SDK supports large language models, multimodal models, embedding models, and speech processing capabilities.

## 📋 Overview

The models module provides standardized interfaces for various AI model providers and types:

## 🎯 Supported Model Categories

### 🤖 Large Language Models (LLM)
- **OpenAI-compatible models**: Any provider following OpenAI API specification
- **Long context models**: Support for extended context windows
- **Multimodal language models**: Text + image processing capabilities
- **Local deployment**: Ollama, vLLM, and other self-hosted solutions

### 🎭 Vision Language Models (VLM)
- **Multimodal understanding**: Process text, images, and documents simultaneously
- **OpenAI-compatible VLMs**: GPT-4V, Claude-3, and compatible models
- **Document analysis**: OCR, table extraction, and visual reasoning

### 🔤 Embedding Models
- **Universal compatibility**: All OpenAI-compatible embedding services
- **Multilingual support**: International language processing
- **Specialized embeddings**: Document, code, and domain-specific embeddings
- **Vector database integration**: Seamless integration with vector stores

### 🎤 Speech Processing Models
- **Text-to-Speech (TTS)**: Multiple provider support
- **Speech-to-Text (STT)**: Real-time and batch processing
- **Voice cloning**: Advanced voice synthesis capabilities
- **Multilingual speech**: Support for multiple languages and accents

## 🏗️ Model Implementation Classes

## 💡 Usage

```python
from nexent.core.models import OpenAIModel

# Initialize OpenAI model
model = OpenAIModel(
    api_key="your-api-key",
    model_name="gpt-4"
)

# Use model for completion
response = model.complete("Hello, world!")
```

## ⚙️ Configuration

Models can be configured through:
- Environment variables
- Configuration files
- Direct parameter passing

For detailed usage examples and API reference, see the SDK documentation.